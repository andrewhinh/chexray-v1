{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# CheXRay\n",
    "* This website first gets your PA and/or Lateral X-Ray views. \n",
    "    * If only one is submitted, the image is copied and sent to the model as the other view (ex. if only a PA view is provided, it will be copied over as a lateral view image and both images will be sent to te model).\n",
    "* It then generates a radiologist report based on those image(s) and corresponding visualizations for each word of where in the image(s) it decided was most important to generate that word.\n",
    "* It then uses the image(s), the generated radiologist report, and the time and date to summarize its findings. \n",
    "* If your condition is known, it generates a list of diseases you may and/or may not have to be checked out for. In addition, it generates visualizations indicating where in the image(s) was the deciding factor.\n",
    "    * If it isn't, the website will direct you towards a practicing radiologist. (Future update) \n",
    "* Note: Although images are saved as files in this website for the purpose of making this website work, they are only within your environment (meaning the website is running in its own world on your computer and there's no way we're getting/keeping it).\n",
    "* If you have any questions (like about how this website works) or concerns, please contact the following email: ajhinh@gmail.com "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "url = \"https://drive.google.com/u/0/uc?export=download&confirm=7rWd&id=1NkCdI2esy90GjZErHMmpokrbRcXI0GYd\"\n",
    "output = 'modules/Fastext_embedd_wordMap.pkl'\n",
    "gdown.download(url, output, quiet=True)\n",
    "\n",
    "url = \"https://drive.google.com/u/0/uc?export=download&confirm=7rWd&id=1K7A9Ow89QwNrohiiCZMwrediAYllgtoJ\"\n",
    "output = 'modules/vocab.pkl'\n",
    "gdown.download(url, output, quiet=True)\n",
    "\n",
    "url = \"https://drive.google.com/u/0/uc?export=download&confirm=7rWd&id=1axHx-EZSr18pwomrNzpsFAnjTEIZnW_z\"\n",
    "output = 'modules/images.zip'\n",
    "gdown.download(url, output, quiet=True)\n",
    "clear_output()\n",
    "\n",
    "url = \"https://drive.google.com/u/0/uc?export=download&confirm=7rWd&id=1Kazb6dY6hTU-NhA1Q4hWeGU2Y442VWbF\"\n",
    "output = 'models/all.0.1.pth'\n",
    "gdown.download(url, output, quiet=True)\n",
    "\n",
    "url = \"https://drive.google.com/u/0/uc?export=download&confirm=7rWd&id=1Q9PPPeDpPaZiciQX2fGLfOETLov04WRa\"\n",
    "output = 'models/allimgcap.0.1.pth'\n",
    "gdown.download(url, output, quiet=True)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling: voila\n",
      "- Writing config: /Users/andrewhinh/anaconda3/etc/jupyter\n",
      "    - Validating...\n",
      "      voila 0.2.4 \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import torch\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from fastai.text.all import *\n",
    "from fastai.tabular.all import *\n",
    "imcap_path = Path('./modules/')\n",
    "\n",
    "!cp str(imcap_path/\"custom.py\") .\n",
    "from custom import *\n",
    "import pickle\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from PIL import Image as im\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "trainval_sample = pd.read_csv(imcap_path/'final_trainval_sample_sample_sample.csv', low_memory=False)\n",
    "\n",
    "trainval_sample = tokenize_df(trainval_sample, 'path')\n",
    "with open(imcap_path/'vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "add_text_to_num(trainval_sample[0], vocab) \n",
    "clear_output(wait=False)\n",
    "\n",
    "original_size=32\n",
    "trans = transforms.Compose([transforms.Resize((original_size,original_size)), transforms.ToTensor()])\n",
    "denorm = transforms.Compose([transforms.functional.to_pil_image])\n",
    "pa_dataset = TestImageCaptionDataset(trainval_sample[0],'images',trans) \n",
    "lat_dataset = TestImageCaptionDataset(trainval_sample[0],'images1',trans) \n",
    "imgcap_collate_func = partial(pad_collate_ImgCap, pad_idx=vocab.index('xxpad'), pad_first=False, transpose=False)\n",
    "def fa_convert(t):\n",
    "    \"A replacement for PyTorch `default_convert` which maintains types and handles `Sequence`s\"\n",
    "    return (default_convert(t) if isinstance(t, _collate_types)\n",
    "            else type(t)([fa_convert(s) for s in t]) if isinstance(t, Sequence)\n",
    "            else default_convert(t))\n",
    "def create_batch(b): return (imgcap_collate_func,fa_convert)[False](b)\n",
    "bs=1\n",
    "pa_dls = DataLoaders.from_dsets(pa_dataset, bs=bs, device=device, create_batch=create_batch, num_workers=0)\n",
    "lat_dls = DataLoaders.from_dsets(lat_dataset, bs=bs, device=device, create_batch=create_batch, num_workers=0)\n",
    "mixed_dls = MixedDL(pa_dls[0], lat_dls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "emb_dim = 300   # 300: pretrined words embedd GLove \n",
    "attention_dim = 512 # encoder_dim tranformed to attention_dim\n",
    "decoder_dim = 512  #  word_emb_dim tranformed to decoder_dim\n",
    "dropout = 0.5\n",
    "encoder_dim = 512 #512 for resnet34 and 2048 for resnet 101 \n",
    "vocab_size = len(vocab)\n",
    "with open(imcap_path/'Fastext_embedd_wordMap.pkl','rb') as f:\n",
    "    embedding = pickle.load(f)\n",
    "###########   Layer Initializations ##########\n",
    "# testing initiation\n",
    "enc = Encoder(14, fine_tune=False)\n",
    "dec = Decoder(attention_dim, \n",
    "              emb_dim, \n",
    "              decoder_dim, \n",
    "              vocab_size, \n",
    "              encoder_dim=encoder_dim, \n",
    "              dropout=0.5, \n",
    "              pretrained_embedding = embedding,\n",
    "              teacher_forcing_ratio=1)\n",
    "# Testing\n",
    "enc = enc.to(device)\n",
    "dec = dec.to(device)\n",
    "arch = Ensemble(enc, dec).to(device)\n",
    "\n",
    "global glb_pa_logits\n",
    "global glb_lat_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# Create our Multi-Modal model\n",
    "multi_model = MultViewCap(arch, arch)\n",
    "# Set weights for each loss\n",
    "pa_w = 0.1\n",
    "lat_w = 0.1\n",
    "pl_w = 0.8\n",
    "\n",
    "# Initialise Loss\n",
    "gb_loss = myGradientBlending(pa_weight=pa_w, lat_weight=lat_w, pa_lat_weight=pl_w, \n",
    "                             loss_scale=1.0, use_cel=True)\n",
    "\n",
    "# Define accuracy weights\n",
    "w_accuracy = partial(weighted_accuracy, w_pa=pa_w, w_lat=lat_w, w_pl=pl_w)\n",
    "bleu1_w = partial(bleu1_weighted, w_pa=pa_w, w_lat=lat_w, w_pl=pl_w)\n",
    "bleu2_w = partial(bleu2_weighted, w_pa=pa_w, w_lat=lat_w, w_pl=pl_w)\n",
    "bleu3_w = partial(bleu3_weighted, w_pa=pa_w, w_lat=lat_w, w_pl=pl_w)\n",
    "bleu4_w = partial(bleu4_weighted, w_pa=pa_w, w_lat=lat_w, w_pl=pl_w)\n",
    "rouge_l_w = partial(rouge_l_weighted, w_pa=pa_w, w_lat=lat_w, w_pl=pl_w)\n",
    "\n",
    "# Setting up Metrics\n",
    "metrics = [topK_accuracy_pa,\n",
    "           topK_accuracy_lat,\n",
    "           topK_accuracy_pl, \n",
    "           w_accuracy,\n",
    "           bleu1_pa,\n",
    "           bleu1_lat,\n",
    "           bleu1_pl,\n",
    "           bleu1_w,\n",
    "           bleu2_pa,\n",
    "           bleu2_lat,\n",
    "           bleu2_pl,\n",
    "           bleu2_w,\n",
    "           bleu3_pa,\n",
    "           bleu3_lat,\n",
    "           bleu3_pl,\n",
    "           bleu3_w,\n",
    "           bleu4_pa,\n",
    "           bleu4_lat,\n",
    "           bleu4_pl,\n",
    "           bleu4_w,\n",
    "           rouge_l_pa,\n",
    "           rouge_l_lat,\n",
    "           rouge_l_pl,\n",
    "           rouge_l_w]\n",
    "\n",
    "# Setting up Callbacks\n",
    "cbs=[CutMixImgCapAll(alpha=1.)]\n",
    "\n",
    "# Model Splitter\n",
    "def split_model_all(arch):\n",
    "    return L(arch.pa_model.encoder, arch.lat_model.encoder, arch.pa_model.decoder, arch.lat_model.decoder).map(params)\n",
    "\n",
    "multi_learn = Learner(mixed_dls, multi_model, gb_loss, opt_func=partial(Adam, betas=(0.8, 0.99)), splitter=split_model_all, cbs=cbs, metrics=metrics)\n",
    "multi_learn = multi_learn.load('allimgcap.0.1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes=[\"Atelectasis\", \n",
    "        \"Cardiomegaly\", \n",
    "        \"Consolidation\", \n",
    "        \"Edema\",\n",
    "        \"Enlarged_Cardiomediastinum\", \n",
    "        \"Fracture\", \n",
    "        \"Lung_Lesion\", \n",
    "        \"Lung_Opacity\", \n",
    "        \"No_Finding\", \n",
    "        \"Pleural_Effusion\",\n",
    "        \"Pleural_Other\",\n",
    "        \"Pneumonia\",\n",
    "        \"Pneumothorax\",\n",
    "        \"Support_Devices\",\n",
    "        \"Other\"]\n",
    "\n",
    "size=32\n",
    "trainval_sample = pd.read_csv(imcap_path/'final_trainval_sample_sample.csv', low_memory=False)\n",
    "trainval_sample = trainval_sample.iloc[:2]\n",
    "df=trainval_sample\n",
    "workers=0\n",
    "seq_len=72\n",
    "from zipfile import ZipFile\n",
    "file_name = str(imgcap_path/'images.zip')\n",
    "destination = './'\n",
    "with ZipFile(file_name) as zf:\n",
    "    zf.extractall(destination)\n",
    "pa_dls_sum = v_dls_test(bs, size, df, 'images', workers)\n",
    "lat_dls_sum = v_dls_test(bs, size, df, 'images1', workers)\n",
    "text_class_dls = tc_dls_test(bs, df, seq_len, vocab, workers) \n",
    "tab_dls = t_dls_test(bs, df, workers)\n",
    "\n",
    "mixed_dls_sum = MixedDLSum(pa_dls_sum[0], lat_dls_sum[0], text_class_dls[0], tab_dls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# Get uni-modal models, ugly but quick way to grab a tabular and vision model\n",
    "model=xresnet18 \n",
    "pa_model = cnn_learner(pa_dls_sum, model).model\n",
    "lat_model = cnn_learner(lat_dls_sum, model).model       \n",
    "arch=AWD_QRNN\n",
    "text_model = text_classifier_learner(text_class_dls, arch).model\n",
    "layers=[500, 250]\n",
    "tab_model = tabular_learner(tab_dls, layers=layers).model\n",
    "\n",
    "# Create our Multi-Modal model\n",
    "multi_model_sum = All(pa_model, lat_model, text_model, tab_model)\n",
    "# Set weights for each loss\n",
    "pa_w = 0.1\n",
    "lat_w = 0.1\n",
    "text_w = 0.1\n",
    "tab_w = 0.1\n",
    "all_w = 0.6\n",
    "\n",
    "# Initialise Loss\n",
    "gb_loss_sum = myGradientBlendingSum(pa_weight=pa_w, \n",
    "                                    lat_weight=lat_w, \n",
    "                                    text_weight=text_w, \n",
    "                                    tab_weight=tab_w, \n",
    "                                    all_weight=all_w, \n",
    "                                    loss_scale=1.0, \n",
    "                                    use_cel=False)\n",
    "\n",
    "# Define accuracy weights\n",
    "w_accuracy_sum = partial(weighted_accuracy_sum, pa_w=pa_w, lat_w=lat_w, text_w=text_w, tab_w=tab_w, all_w=all_w)\n",
    "w_ap = partial(weighted_ap, pa_w=pa_w, lat_w=lat_w, text_w=text_w, tab_w=tab_w, all_w=all_w)\n",
    "w_roc = partial(weighted_roc, pa_w=pa_w, lat_w=lat_w, text_w=text_w, tab_w=tab_w, all_w=all_w)\n",
    "\n",
    "metrics_sum = [pa_accuracy, lat_accuracy, text_accuracy, tab_accuracy, all_accuracy, w_accuracy_sum,\n",
    "               pa_ap, lat_ap, text_ap, tab_ap, all_ap, w_ap,\n",
    "               pa_roc, lat_roc, text_roc, tab_roc, all_roc, w_roc]\n",
    "\n",
    "# Model Splitter: Fix for text model\n",
    "\n",
    "cbs_sum=[CutMixAll(alpha=1.)]\n",
    "\n",
    "multi_learn_sum = Learner(mixed_dls_sum, multi_model_sum, gb_loss_sum, splitter=split_model_sum, cbs=cbs_sum, metrics=metrics_sum)\n",
    "multi_learn_sum = multi_learn_sum.load('all.0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "prod_path = Path('./sample/')\n",
    "btn_run = widgets.Button(description='Classify')\n",
    "def on_click_classify(change):\n",
    "    pa_img = PILImage.create(pa_btn_upload.data[-1])\n",
    "    with out_pl: display(pa_img.to_thumb(128,128))\n",
    "        \n",
    "    lat_img = PILImage.create(lat_btn_upload.data[-1])\n",
    "    with out_pl: display(lat_img.to_thumb(128,128))\n",
    "                                                                                 \n",
    "    if pa_btn_upload.data[-1]!=[]:\n",
    "        with open(prod_path/'pa.jpg', 'wb') as f: \n",
    "            f.write(pa_btn_upload.value[list(pa_btn_upload.value.keys())[0]]['content'])\n",
    "    else:\n",
    "        with open(prod_path/'pa.jpg', 'wb') as f: \n",
    "            f.write(lat_btn_upload.value[list(lat_btn_upload.value.keys())[0]]['content'])\n",
    "\n",
    "    if lat_btn_upload.data[-1]!=[]:\n",
    "        with open(prod_path/'lat.jpg', 'wb') as f: \n",
    "            f.write(lat_btn_upload.value[list(lat_btn_upload.value.keys())[0]]['content'])\n",
    "    else:\n",
    "        with open(prod_path/'lat.jpg', 'wb') as f: \n",
    "            f.write(pa_btn_upload.value[list(pa_btn_upload.value.keys())[0]]['content']) \n",
    "\n",
    "    with open(prod_path/'report.txt', 'wb') as f: f.write(btn_upload.value[list(btn_upload.value.keys())[0]]['content'])\n",
    "    with open(prod_path/'report.txt') as report: textreport = report.read()\n",
    "\n",
    "    \"\"\"\n",
    "    pa_img = plt.imread(prod_path/'pa.jpg')\n",
    "    lat_img = plt.imread(prod_path/'lat.jpg')\n",
    "    pa_img = torch.stack((torch.as_tensor(pa_img,), torch.as_tensor(pa_img,), torch.as_tensor(pa_img,)), axis=0)\n",
    "    lat_img = torch.stack((torch.as_tensor(lat_img,), torch.as_tensor(lat_img,), torch.as_tensor(lat_img,)), axis=0)\n",
    "    pa_img = torch.from_numpy(np.array(pa_img)).float()\n",
    "    lat_img = torch.from_numpy(np.array(lat_img)).float()\n",
    "\n",
    "    caps, alphas = beam_search_all(multi_learn.model, pa_img, lat_img, vocab, 5)\n",
    "    caps = [vocab[x] for x in caps[0]]\n",
    "    textreport = ' '.join(caps)\n",
    "\n",
    "    visualize_att_all('pa.jpg', 'lat.jpg', cap, alphas, prod_path)\n",
    "    \"\"\"\n",
    "\n",
    "    from datetime import datetime\n",
    "    tabcols = ['StudyTime', 'StudyDate', 'Seconds', 'Minutes', 'FracSec']\n",
    "    tab = pd.DataFrame(columns=tabcols)\n",
    "    tab.loc[0, 'StudyTime']=float(str(datetime.now().hour)+str(datetime.now().minute)+str(datetime.now().second)+'.'+str(datetime.now().microsecond))\n",
    "    tab.iloc[0, 1]=float(str(datetime.now().year)+str(datetime.now().month)+str(datetime.now().day))\n",
    "    tab.iloc[0, 2]=datetime.now().second\n",
    "    tab.iloc[0, 3]=datetime.now().minute\n",
    "    tab.iloc[0, 4]=datetime.now().microsecond\n",
    "\n",
    "    size=64\n",
    "    df = pd.read_csv(imcap_path/'final_trainval_sample_sample.csv', low_memory=False)\n",
    "    pa = prod_path/'pa.jpg'\n",
    "    lat = prod_path/'lat.jpg'\n",
    "    text = textreport\n",
    "    tab = tab.iloc[0, :]\n",
    "    label = df.loc[0, classes]\n",
    "    new, preds = predict_sum(multi_learn_sum, pa, lat, text, tab, label, df, tabcols, size)\n",
    "    idx = (torch.tensor(preds>0.5)[0] == True).nonzero().flatten().tolist()\n",
    "    \n",
    "    b=[]\n",
    "    if idx==[]:\n",
    "        b.append(10)\n",
    "        idx.append('Other')\n",
    "    else:\n",
    "        for i in range(len(idx)):\n",
    "            b.append(preds[0][idx[i]])\n",
    "            idx[i]=classes[idx[i]]\n",
    "\n",
    "    temp = dict(zip(idx, b))\n",
    "    temp = dict(sorted(temp.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    idx = list(temp.keys())\n",
    "    b = list(temp.values())\n",
    "    nograd = False\n",
    "    thresh=0.5\n",
    "    if b[0]<thresh or (len(idx)==1 and idx[0]==\"Other\"):\n",
    "        lbl_pred.value += \"Your condition cannot be determined. Please contact your radiologist for further consultation.\"\n",
    "        nograd = True\n",
    "    elif b[-1]>=thresh:\n",
    "        lbl_pred.value += \"You most likely need to get checked out for the following: \"\n",
    "        for finding in range(len(idx)):\n",
    "            if finding!=len(idx)-1:\n",
    "                lbl_pred.value += idx[finding] + f\"({b[finding]*100}% confident), \"\n",
    "            else:\n",
    "                lbl_pred.value += idx[finding] + f\"({b[finding]*100}% confident).\"\n",
    "    else:\n",
    "        lbl_pred.value += \"You most likely need to get checked out for the following: \"\n",
    "        where_stop = len(idx)-1\n",
    "        for finding in range(len(idx)):\n",
    "            if finding<where_stop:\n",
    "                if b[finding]>=thresh:\n",
    "                    if b[finding+1]<thresh:\n",
    "                        where_stop=finding+1\n",
    "                        lbl_pred.value += idx[finding] + f\" ({b[finding]*100}% confident);\" \n",
    "                        break\n",
    "                    else:\n",
    "                        lbl_pred.value += idx[finding] + f\" ({b[finding]*100}% confident), \" \n",
    "                else:\n",
    "                    where_stop=finding\n",
    "                    break\n",
    "        lbl_pred.value += \"You most likely don't need to get checked out for the following: \"\n",
    "        for finding in range(where_stop, len(idx)):\n",
    "            if finding!=len(idx)-1:\n",
    "                lbl_pred.value += idx[finding] + f\" ({b[finding]*100}% confident), \"\n",
    "            else:\n",
    "                lbl_pred.value += idx[finding] + f\" ({b[finding]*100}% confident).\"\n",
    "    \n",
    "    if not nograd:\n",
    "        bs=1\n",
    "        df=new.iloc[:1]\n",
    "        pa = v_dls_new(bs, size, df, 'images', workers)\n",
    "        lat = v_dls_new(bs, size, df, 'images1', workers)\n",
    "        pa_img = pa.one_batch()[0]\n",
    "        pa_img = torch.tensor(torch.cat((pa_img,)*3, axis=1))\n",
    "        lat_img = lat.one_batch()[0]\n",
    "        lat_img = torch.tensor(torch.cat((lat_img,)*3, axis=1))      \n",
    "\n",
    "        temp = (torch.tensor(preds>0.5)[0] == True).nonzero().flatten().tolist()\n",
    "        return_grad(multi_learn_sum, multi_learn_sum.model.pa_model, pa_img, 0, prod_path, size, temp)\n",
    "        return_grad(multi_learn_sum, multi_learn_sum.model.lat_model, lat_img, 1, prod_path, size, temp)\n",
    "\n",
    "        for i in range(1, len(idx)+1):\n",
    "            pa_grad = plt.imread(prod_path/str(\"pa_gradcam\"+str(i)+\".png\"))\n",
    "            pa_grad = im.fromarray((pa_grad * 255).astype(np.uint8))\n",
    "            with out_pl: display(pa_grad.to_thumb(214,256)) \n",
    "            lat_grad = plt.imread(prod_path/str(\"lat_gradcam\"+str(i)+\".png\"))\n",
    "            lat_grad = im.fromarray((lat_grad * 255).astype(np.uint8))\n",
    "            with out_pl: display(lat_grad.to_thumb(214,256))\n",
    "\n",
    "btn_run.on_click(on_click_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b563229aa8443994653de2924c7ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=\"Upload your patient's PA, AP, or any closely related view.\"), FileUpload(value={},â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_output\n",
    "from ipywidgets import *\n",
    "pa_btn_upload = widgets.FileUpload()\n",
    "lat_btn_upload = widgets.FileUpload()\n",
    "btn_upload = widgets.FileUpload()\n",
    "out_pl = widgets.Output()\n",
    "lbl_pred = widgets.Label()\n",
    "\n",
    "VBox([widgets.Label(\"Upload your patient's PA, AP, or any closely related view.\"),\n",
    "      pa_btn_upload, \n",
    "      widgets.Label(\"Then upload your patient's lateral, LL, or any closely related view.\"), \n",
    "      lat_btn_upload, btn_upload, btn_run, out_pl, lbl_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
